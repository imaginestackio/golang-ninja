# Cloud Deployment

In this chapter, we will learn about cloud deployment, specifically using AWS as the cloud provider. We will look at some of the infrastructure services provided by AWS and how to use them. We will learn about using and writing code for creating the different AWS infrastructure services using an open source tool called Terraform. Understanding the cloud and how cloud deployment works has become a necessity for developers nowadays rather than an exception. Gaining a good understanding of the different aspects of cloud deployment will allow you to think outside the box about how your application should run in the cloud.

Upon completion of this chapter, we will have learned about the following key topics:

-   Learning basic AWS infrastructure
-   Understanding and using Terraform
-   Writing Terraform for local and cloud deployment
-   Deploying to AWS Elastic Container Service

The end goal of this chapter is to provide you with some knowledge about the cloud and how to perform certain basic operations for deploying applications to the cloud.

Bookmark

# Technical requirements

All the source code explained in this chapter can be checked out at [https://github.com/ImagineDevOps DevOps/Full-Stack-Web-Development-with-Go/tree/main/chapter14](https://github.com/ImagineDevOps DevOps/Full-Stack-Web-Development-with-Go/tree/main/chapter14).

This chapter uses AWS services, so you are expected to have an AWS account. AWS provides a Free Tier for new user registration; more information can be found at [https://aws.amazon.com/free](https://aws.amazon.com/free).

Note

Using any kind of AWS services will incur a cost. Please read and inform yourself before using the service. We highly recommend reading what is available on the Free Tier on the AWS website.

Bookmark

# AWS refresher

**AWS** stands for **Amazon Web Services** and belongs to Amazon, which provides the e-commerce platform [amazon.com.au](http://amazon.com.au). AWS provides services that allow organizations to run their applications in a complete infrastructure without owning any of the hardware required.

The AWS brand is a household name for developers and almost all developers have some basic direct/indirect exposure to using AWS tools or its services. In this section, we will look at some services provided by AWS as a refresher.

The question that comes to our mind is, why bother using services such as AWS? _Figure 14__.1_ summarizes the answer nicely. AWS provides services that are available across different continents of the world and ready to be used by organizations to fulfill their needs. ImagineDevOps  that your organization has customers across different continents. How much easier would it be to run your application on different continents without having the burden of investing in hardware on each of those continents?

![Figure 14.1: Global AWS Regions](https://static.packt-cdn.com/products/9781803234199/graphics/image/Figure_14.01_B18295.jpg)

Figure 14.1: Global AWS Regions

In the next section, we will look at the basic service provided by AWS called AWS EC2, which provides computing resources.

## Amazon Elastic Compute Cloud

Amazon **Elastic Compute Cloud** (**EC2**) is the basic computing resource for developers to run their applications on. You can think of EC2 as a virtual computer on Amazon infrastructure somewhere on the internet that runs your application. You can select from a number of computer configurations that you want to run your application on, from a small 512-MB memory to a gigantic 384-GB memory computer with different configurations of storage. _Figure 14__.2_ shows the Instance Type Explorer that can be accessed using the following URL: [https://aws.amazon.com/ec2/instance-explorer/](https://aws.amazon.com/ec2/instance-explorer/).

![Figure 14.2: Instance Type Explorer](https://static.packt-cdn.com/products/9781803234199/graphics/image/Figure_14.02_B18295.jpg)

Figure 14.2: Instance Type Explorer

In the next section, we will look at another AWS resource related to computing that is super important for applications, and that is storage.

Storage

Computing power is great for running applications, but applications require long-term storage to store data such as log files and databases. There are a number of different kinds of storage provided by AWS. For example, _Figure 14__.3_ shows the **Elastic Block Store** (**EBS**), which is a block storage service. This block storage is like the normal storage that you have on your local computer and is offered as a hard drive or a **solid-state** **drive** (**SSD**).

![Figure 14.3: EBS](https://static.packt-cdn.com/products/9781803234199/graphics/image/Figure_14.03_B18295.jpg)

Figure 14.3: EBS

The amazing thing about having this kind of storage is its elastic nature – what this means is you can increase or decrease the size of storage anytime you need without the worry of adding new hardware. ImagineDevOps  what would happen if you were running out of hard drive space on your local computer. You would need to buy a new hard drive and install and configure it, none of which is required when you use the AWS storage service. Attaching storage to the EC2 instance of your choice enables your application to run and store data in the cloud.

We will look at another AWS service that is as important as the one that we have just discussed: networking.

## Virtual Private Cloud

Now that your application is running in its own virtual computer, complete with storage, the next question is how we configure a network in AWS so that users can access the application. This is called a **Virtual Private Cloud** (**VPC**). Think about a VPC as your own network setup, but without cables – everything is configured and run using software. _Figure 14__.4_ shows the powerful capability of a VPC, enabling you to connect different networks configured in different Regions.

Think of a Region as the physical location where AWS stores its hardware, and if you run your applications in different physical locations, you are able to connect them using a VPC.

![Figure 14.4: Virtual Private Networking](https://static.packt-cdn.com/products/9781803234199/graphics/image/Figure_14.04_B18295.jpg)

Figure 14.4: Virtual Private Networking

You have full control to configure the network of each Region your application is running on, how these Regions communicate with your own network, and how your application will be accessible via the public internet.

In the next section, we will look at another important service that a lot of applications require which is storing data in a database.

## Database storage

No matter what kind of applications you are building, you will require a database to store data, and this requires a database server to be running. AWS provides different database services ranging from those that store small amounts of data to massively distributed databases across different continents. One of these services is called Amazon **Relational Database Service** (**RDS**), a managed service to set up, scale, and operate databases.

The databases that RDS can support are MySQL, PostgreSQL, MariaDB, Oracle, and SQL Server. _Figure 14__.5_ outlines the features provided by RDS.

![Figure 14.5: RDS](https://static.packt-cdn.com/products/9781803234199/graphics/image/Figure_14.05_B18295.jpg)

Figure 14.5: RDS

## Elastic Container Service

In [_Chapter 13_](https://subscription.imaginedevops.io/book/web-development/9781803234199/2B18295_13.xhtml#_idTextAnchor261), _Dockerizing an Application_, we learned how to create Docker images to package our application so it can run as a container. Packaging applications as Docker images allows us to run our application in any kind of environment, from a local machine to the cloud. AWS provides a related service called **Elastic Container** **Service** (**ECS**).

ECS helps us to deploy, manage, and scale out applications that have been built as containers. A key scaling feature of ECS is the ability to scale your application using the Application Auto Scaling capability. This feature allows developers to scale applications based on certain conditions, such as the following:

-   **Step scaling**: This means scaling an application based on the breach of an alarm
-   **Scheduled scaling**: This is scaling based on a predetermined time

## AWS tools

AWS provides different ways to use its services, including a web user interface and the **command-line interface** (**CLI**). The main page of the web UI can be seen in _Figure 14__.6_. You will need to register for an AWS account first before using any of the AWS tools.

The UI is a very good place to start exploring the different services and go through some sample tutorials to get a better understanding of each service.

![Figure 14.6: AWS web UI](https://static.packt-cdn.com/products/9781803234199/graphics/image/Figure_14.06_B18295.jpg)

Figure 14.6: AWS web UI

The other AWS tool that is used to interact with the services is the CLI, which needs to be installed locally ([https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html)). The CLI makes it easier to interact with the AWS services than the web UI. If you have installed it locally, when you run `aws` from your terminal, you will see the following output:

```markup
usage: aws [options] <command> <subcommand> [<subcommand> ...] [parameters]
To see help text, you can run:
  aws help
  aws <command> help
  aws <command> <subcommand> help
aws: error: the following arguments are required: command
```

In the next section, we will look at how to use some of the features described here to deploy our application in AWS.

Bookmark

# Understanding and using Terraform

In this section, we will look at another tool that makes it easier for us to work with AWS services: Terraform. In the previous section, we learned that AWS provides tools of its own, which is great for small tasks, but once you start combining the different services it becomes harder to use them.

## What is Terraform?

Terraform ([https://www.terraform.io/](https://www.terraform.io/)) is an open source tool that provides **infrastructure as code** (**IaC**). What this means is you write code to define what kind of service you want to use and how you want to use it, and this way, you can combine and link the different services together as a single piece. This makes it easy for you as a developer to run and destroy infrastructure as a unit instead of separate fragments.

The other benefit that Terraform provides is the ability to version control the infrastructure code like normal application code, where it goes through the normal review process, including the peer review process and also unit testing, before deploying the infrastructure to production. With this, your application and infrastructure will now go through the same development process, which is trackable.

## Installing Terraform

The Terraform installation process is straightforward: you can find a complete set of instructions for your operating system in the HashiCorp documentation at [https://www.terraform.io/downloads](https://www.terraform.io/downloads).

For example, when writing this book we are using an Ubuntu-based distro, so we download the AMD64 binary from [https://releases.hashicorp.com/terraform/1.3.0/terraform\_1.3.0\_linux\_amd64](https://releases.hashicorp.com/terraform/1.3.0/terraform_1.3.0_linux_amd64)`.zip` and include the Terraform directory into our `PATH`, as in the following snippet. The directory added to the `PATH` variable environment is a temporary solution for the terminal that you are using. In order to store it, you need to put it as part of your shell script (for Linux, if you are using Bash, you can add this to your `.``bashrc` file):

```markup
export PATH=$PATH:/home/user/Downloads/
```

To test whether the installation was successful, open the terminal and execute `Terraform`:

```markup
Terraform
```

You should get the following output:

```markup
Usage: terraform [global options] <subcommand> [args]
The available commands for execution are listed below.
The primary workflow commands are given first, followed by
less common or more advanced commands.
Main commands:
  init          Prepare your working directory for other
                commands
  ...
All other commands:
  console       Try Terraform expressions at an interactive
                command prompt
  fmt           Reformat your configuration in the standard
                style
  ...
```

For detailed information on how to install Terraform for your environment, see [https://developer.hashicorp.com/terraform/tutorials/aws-get-started/install-cli](https://developer.hashicorp.com/terraform/tutorials/aws-get-started/install-cli).

Now that we have completed the Terraform installation, we will learn how to use some of the basic commands available in Terraform. The commands will enable you to jumpstart your journey into the world of cloud deployment.

Bookmark

# Terraform basic commands

In this section, we will learn some basic Terraform commands that are often used when writing code. We will also examine concepts that are relevant to Terraform.

## The init command

Every time we start writing Terraform code, the first command that we run is `terraform init`. This command prepares all the necessary dependencies required to run the code locally. The command performs the following steps:

1.  Downloads all the necessary modules that are used in the code.
2.  Initializes plugins that are used in the code. For example, if the code is deployed on AWS it will download the AWS plugins.
3.  Creates a file called a lock file that registers the different dependencies and versions that are used by the code.

To gain a better understanding of the previous steps, let’s run the command. Open the terminal and change to the `chapter14/simple` directory, and execute the following command:

```markup
terraform init
```

You will see an output as follows:

```markup
Initializing the backend...
Initializing provider plugins...
- Finding kreuzwerker/docker versions matching "~> 2.16.0"...
- Installing kreuzwerker/docker v2.16.0...
- Installed kreuzwerker/docker v2.16.0 (self-signed, key ID BD080C4571C6104C)
...
```

Once the `init` process is complete, your directory will look like the following:

```markup
.
├── main.tf
├── .terraform
│   └── providers
│       └── registry.terraform.io
│           └── kreuzwerker
│               └── docker
│                   └── 2.16.0
│                       └── linux_amd64
│                           ├── CHANGELOG.md
│                           ├── LICENSE
│                           ├── README.md
│                           └── terraform-provider-docker_v2.16.0
├── .terraform.lock.hcl
└── versions.tf
```

The `.terraform` directory contains the dependencies that are specified in the code. In this example, it uses the `kreuzwerker/docker` plugin, which is used to run Docker containers.

The `.terraform.lock.hcl` file contains the version information of the dependencies, and it looks like the following:

```markup
# This file is maintained automatically by "terraform
# init".
# Manual edits may be lost in future updates.
provider "registry.terraform.io/kreuzwerker/docker" {
 version     = "2.16.0"
 constraints = "~> 2.16.0"
 hashes = [
   "h1:OcTn2QyCQNjDiJYy1vqQFmz2dxJdOF/2/HBXBvGxU2E=",
   ...
 ]
}
```

## The plan command

The `plan` command is used to help us understand the execution plan that Terraform will be doing. This is a very important feature as it gives us visibility of what changes will be performed to our infrastructure. This will give us a better understanding of which parts of the infrastructure will be impacted by the code. Unlike tools such as Chef or Ansible, Terraform is interesting in that it will tend towards a target state and only make the changes necessary to reach it. For example, if you had a target of five EC2 instances but Terraform only knew of three, it would take the steps needed to reach that target of five.

Open the terminal, change to the `chapter14/simple` directory, and execute the following command:

```markup
terraform plan
```

You will get the following output:

```markup
...
Terraform will perform the following actions:
  # docker_container.nginx will be created
  + resource "docker_container" "nginx" {
      + attach           = false
      + bridge           = (known after apply)
      + command          = (known after apply)
      + container_logs   = (known after apply)
      + entrypoint       = (known after apply)
      + env              = (known after apply)
      + exit_code        = (known after apply)
      ...
      + remove_volumes   = true
      + restart          = "no"
      + rm               = false
      + security_opts    = (known after apply)
      + shm_size         = (known after apply)
      + start            = true
      + stdin_open       = false
      + tty              = false
      + healthcheck {
          + interval     = (known after apply)
          + retries      = (known after apply)
          + start_period = (known after apply)
          + test         = (known after apply)
          + timeout      = (known after apply)
        }
      + labels {
          + label = (known after apply)
          + value = (known after apply)
        }
      + ports {
          + external = 8000
          + internal = 80
          + ip       = "0.0.0.0"
          + protocol = "tcp"
        }
    }
  # docker_image.nginx will be created
  + resource "docker_image" "nginx" {
      + id           = (known after apply)
      ...
      + repo_digest  = (known after apply)
    }
Plan: 2 to add, 0 to change, 0 to destroy.
...
```

The output shows that there will be `2` things added and `0` operations for changing or destroying, which tells us that this is the first time we are running the code or it’s still fresh.

### The apply command

The normal process of running Terraform is that after `init`, we run `apply` (however, if we are not sure about the impact, we use the `plan` command as shown previously). Open the terminal, change to the `chapter14/simple` directory, and execute the following command:

```markup
terraform apply –auto-aprove
```

You will get the following output:

```markup
...
Terraform will perform the following actions:
  # docker_container.nginx will be created
  + resource "docker_container" "nginx" {
      + attach           = false
      + bridge           = (known after apply)
      ...
    }
  # docker_image.nginx will be created
  + resource "docker_image" "nginx" {
      + id           = (known after apply)
      ...
    }
Plan: 2 to add, 0 to change, 0 to destroy.
docker_image.nginx: Creating...
docker_image.nginx: Still creating... [10s elapsed]
docker_image.nginx: Creation complete after 17s [id=sha256:2d389e545974d4a93ebdef09b650753a55f72d1ab4518d17a 30c0e1b3e297444nginx:latest]
docker_container.nginx: Creating...
docker_container.nginx: Creation complete after 2s [id=d0c94bd4 b548e6a19c3afb907a777bcb602e965bc05db8ef6d0d380601bb0694]
Apply complete! Resources: 2 added, 0 changed, 0 destroyed.
```

As seen in the output, the `nginx` container will be downloaded (if it does not exist as yet) and then run. Once the command is successfully run you can test it by opening your browser and accessing http://localhost:8080. You will see something like _Figure 14__.7_.

![Figure 14.7: nginx running in a container](https://static.packt-cdn.com/products/9781803234199/graphics/image/Figure_14.07_B18295.jpg)

Figure 14.7: nginx running in a container

## The destroy command

The last command that we will look at is `destroy`. As the name implies, it is used to destroy the infrastructure that was created using the `apply` command. Use this command with caution if you are unsure about the impact of the code on your infrastructure. Use the `plan` command before running this to get better visibility of what will be removed from the infrastructure.

Open the terminal and run the following command from the `chapter14/simple` directory:

```markup
Terraform destroy –auto-approve
```

You will get the following output:

```markup
docker_image.nginx: Refreshing state... [id=sha256:2d389e545974d4a93ebdef09b650753a55f72d1ab4518d17a30c 0e1b3e297444nginx:latest]
docker_container.nginx: Refreshing state... [id=9c46cff8 1a27edb6aba08a448d715599c644aaa79b192728016db0d903da9fb0]
...
Terraform will perform the following actions:
  # docker_container.nginx will be destroyed
  - resource "docker_container" "nginx" {
      - attach            = false -> null
      - command           = [
          - "nginx",
          - "-g",
          - "daemon off;",
        ] -> null
      - cpu_shares        = 0 -> null
      …
    }
  # docker_image.nginx will be destroyed
  - resource "docker_image" "nginx" {
      - id           =
         "sha256:2d389e545974d4a93ebdef09b650753a55f7
         2d1ab4518d17a30c0e1b3e297444nginx:latest" ->
         null
      - keep_locally = false -> null
      - latest       =
         "sha256:2d389e545974d4a93ebdef09b650753a55f72
          d1ab4518d17a30c0e1b3e297444" -> null
      - name         = "nginx:latest" -> null
      - repo_digest  =
          "nginx@sha256:0b970013351304af46f322da126351
           6b188318682b2ab1091862497591189ff1" -> null
    }
Plan: 0 to add, 0 to change, 2 to destroy.
docker_container.nginx: Destroying... [id=9c46cff81a27edb6aba 08a448d715599c644aaa79b192728016db0d903da9fb0]
docker_container.nginx: Destruction complete after 1s
docker_image.nginx: Destroying... [id=sha256:2d389e545974d4a93 ebdef09b650753a55f72d1ab4518d17a30c0e1b3e297444nginx:latest]
docker_image.nginx: Destruction complete after 0s
Destroy complete! Resources: 2 destroyed.
```

In the output, we can see that there are `2` infrastructures that are destroyed – one is the container removed from memory, and the other is the removal of the image from the local Docker registry.

The `–auto-approve` command is used to automatically approve the steps; normally, without using this, Terraform will stop execution and ask the user to enter `Yes` or `No` to continue at each step. This is a precautionary measure to ensure that the user does indeed want to destroy the infrastructure.

In the next section, we will look at writing Terraform code and how it uses providers. We will look at a few Terraform examples to get an understanding of how it works to spin up different AWS infrastructure services for deploying applications.

Bookmark

# Coding in Terraform

HashiCorp, the creator of Terraform, created **HashiCorp configuration language** (**HCL**), which is used in writing Terraform code. HCL is a functional programming language with features such as loops, if statements, variables, and logic flow that are normally found in programming languages. Complete in-depth HCL documentation can be found at [https://www.terraform.io/language/](https://www.terraform.io/language/).

## Providers

The reason why Terraform is so widely used is the number of extensions that are available from the company and open source communities; these extensions are called providers. A provider is a piece of software that interacts with the different cloud providers and other resources in the cloud. We will look at Terraform code to understand more about providers. The following code snippets can be found inside the `chapter14/simple` directory:

```markup
terraform {
 required_providers {
   docker = {
     source = "kreuzwerker/docker"
     version = "~> 2.16.0"
   }
 }
}
resource "docker_image" "nginx" {
 name         = "nginx:latest"
 keep_locally = false
}
resource "docker_container" "nginx" {
 image = docker_image.nginx.name
 name  = "hello-terraform"
 ports {
   internal = 80
   external = 8000
 }
}
```

The `resource` block in the code can be used to declare infrastructure or an API. In this example, we are using Docker, specifically, `docker_image` and `docker_container`. When Terraform runs the code it detects the `required_providers` block, which is used to define a provider. A provider is an external module that the code will be using, and this will be automatically downloaded by Terraform from a central repository. In our example, the provider that we are using is the `kreuzwerker/docker` Docker provider. More information on this provider can be found at the following link: [https://registry.terraform.io/providers/kreuzwerker/docker/](https://registry.terraform.io/providers/kreuzwerker/docker/).

Open the terminal, make sure you are inside the `chapter14/simple` directory, and run the following command:

```markup
terraform init
```

You will see the following output in your terminal:

```markup
Initializing the backend...
Initializing provider plugins...
- Finding kreuzwerker/docker versions matching "~> 2.16.0"...
- Installing kreuzwerker/docker v2.16.0...
- Installed kreuzwerker/docker v2.16.0 (self-signed, key ID BD080C4571C6104C)
...
```

Terraform downloads the provider and stores it inside the `chapter14/simple/.terraform` folder. Now, let’s run the sample code and see what we get, by running the following command in the same terminal:

```markup
terraform apply -auto-approve
```

You will see the following output:

```markup
…
  # docker_container.nginx will be created
  + resource "docker_container" "nginx" {
      + attach           = false
      ...
    }
  # docker_image.nginx will be created
  + resource "docker_image" "nginx" {
      + id           = (known after apply)
     …
    }
Plan: 2 to add, 0 to change, 0 to destroy.
  …
docker_image.nginx: Creation complete after 22s [id=sha256:2d389e545974d4a93ebdef09b650753a55f72d1ab4518d17a 30c0e1b3e297444nginx:latest]
docker_container.nginx: Creating...
docker_container.nginx: Creation complete after 2s [id=b860780 af83a4c719a916b87171d96801cc2243a61242354815f6d82dc6a5e40]
```

Open your browser and go to http://localhost:8000. You will see something like _Figure 14__.7_.

Terraform downloads the `nginx` Docker image automatically to your local machine and runs the `nginx` container using the port defined in the `ports` code block (port `8000`). To destroy the running container and delete the image locally from the Docker registry, all you have to do is run the following command:

```markup
terraform destroy -auto-approve
```

If you compare the steps involved to do the same thing manually using the Docker command, it is more involved and error-prone; writing it in Terraform makes it much easier to run and remove containers with a single command.

In the next section, we will explore more examples to better understand how to use Terraform for deploying applications.

Bookmark

# Terraform examples

In the following sections, we will look at different ways we can use Terraform, such as pulling images from GitHub and running them locally, or building and publishing Docker images.

Note

Make sure every time you run Terraform examples that create AWS resources to remember to destroy the resources using the `terraform` `destroy` command.

All resources created in AWS incur charges, and by destroying them, you will ensure there will be no surprise charges.

## Pulling from GitHub Packages

The example code for this section can be found inside the `chapter14/github` folder. The following snippet is from `pullfromgithub.tf`:

```markup
#script to pull chapter12 image and run it locally
#it also store the image locally
terraform {
 required_providers {
   docker = {
     source  = "kreuzwerker/docker"
     version = "~> 2.13.0"
   }
 }
}
data "docker_registry_image" "github" {
 name = "ghcr.io/nanikjava/golangci/chapter12:latest"
}
resource "docker_image" "embed" {
 ...
}
resource "docker_container" "embed" {
 ...
}
```

The main objective of the code is to download the Docker image that we built in [_Chapter 12_](https://subscription.imaginedevops.io/book/web-development/9781803234199/2B18295_12.xhtml#_idTextAnchor241)_, Building Continuous Integration_. Once the Docker image is downloaded, it will be run locally. Open your terminal, make sure you are inside the `chapter14/github` directory, and run the following command:

```markup
terraform init
```

Then run the following command:

```markup
terraform apply -auto-approve
```

You will see output in your terminal that looks like the following:

```markup
…
data.docker_registry_image.github: Reading...
data.docker_registry_image.github: Read complete after 1s [id=sha256:a355f55c33a400290776faf20b33d45096eb19a6431fb 0b3f723c17236e8b03e]
…
  # docker_container.embed will be created
  + resource "docker_container" "embed" {
      + attach           = false
     …
      + ports {
          + external = 3333
          + internal = 3333
          …
        }
    }
  # docker_image.embed will be created
  + resource "docker_image" "embed" {
      …
      + name         =
         "ghcr.io/nanikjava/golangci/chapter12:latest"
       …
    }
Plan: 2 to add, 0 to change, 0 to destroy.
… [id=sha256:684e34e77f40ee1e75bfd7d86982a4f4fae1dbea3286682af 3222a270faa49b7ghcr.io/nanikjava/golangci/chapter12:latest]
docker_container.embed: Creation complete after 7s [id=f47d1ab90331dd8d6dd677322f00d9a06676b71dda3edf2cb2e66 edc97748329]
Apply complete! Resources: 2 added, 0 changed, 0 destroyed.
```

Open your browser and go to http://localhost:3333. You will see the login page of the sample app.

The code uses the same `docker` provider that we discussed in the previous section, and we use a new `docker_registry_image` command to specify the address to download the Docker image from, in this case from the `ghcr.io/nanikjava/golangci/chapter12:latest` GitHub package.

The other HCL feature we are using is the `data` block, as shown here:

```markup
...
data "docker_registry_image" "github" {
 name = "ghcr.io/nanikjava/golangci/chapter12:latest"
}
...
```

The `data` block works similarly to `resource`, except it is only used for reading values and not creating or destroying resources or to get data that will be used internally as configuration to another resource. In our sample, it is used by the `docker_image` resource, as shown here:

```markup
resource "docker_image" "embed" {
 keep_locally = true
 name         = "${data.docker_registry_image.github.name}"
}
```

## AWS EC2 setup

In the previous examples, we looked at using the Docker provider to run Docker containers locally. In this example, we will look at creating AWS resources, specifically EC2 instances. An EC2 instance is basically a virtual machine that can be initialized with a certain configuration to run in the cloud to host your application.

In order to create resources in AWS, you will first need to already have an AWS account. If you don’t have an AWS account, you can create one at [https://aws.amazon.com/](https://aws.amazon.com/). Once you have your AWS account ready, log in to the AWS website, and in the main console (_Figure 14__.6_) web page, click on your name on the right side and it will display a drop-down menu, as shown in _Figure 14__.8_. Then click on **Security credentials**.

![Figure 14.8: Security credentials option](https://static.packt-cdn.com/products/9781803234199/graphics/image/Figure_14.08_B18295.jpg)

Figure 14.8: Security credentials option

Your browser will now show the **identity and access management** (**IAM**) page, as shown in _Figure 14__.9_. Select the **Access keys (access key ID and secret access key)** option. Since you haven’t created any key, it will be empty. Click on the **Create New Access Key** button and follow the instructions to create a new key.

![Figure 14.9: Access keys section](https://static.packt-cdn.com/products/9781803234199/graphics/image/Figure_14.09_B18295.jpg)

Figure 14.9: Access keys section

Once you complete the steps you will get two keys – an Access Key ID and Secret Access Key. Keep these keys safe as they are used like a username and password combination you use to create resources in AWS infrastructure.

Now that you have the keys required, you can now open a terminal and change into the `chapter14/simpleec2` directory, and run the example as follows:

```markup
terraform init
```

Next, run the following command to create the EC2 instance:

```markup
terraform apply  -var="aws_access_key=xxxx" -var="aws_secret_key=xxx" -auto-approve
```

Once completed you will see the output as follows:

```markup
...
Terraform will perform the following actions:
  # aws_instance.app_server will be created
  + resource "aws_instance" "app_server" {
      + ami = "ami-0ff8a91507f77f867"
      ...
    }
  # aws_subnet.default-subnet will be created
  + resource "aws_subnet" "default-subnet" {
      ...
    }
  # aws_vpc.default-vpc will be created
  + resource "aws_vpc" "default-vpc" {
      + arn                      = (known after apply)
      ...
    }
Plan: 3 to add, 0 to change, 0 to destroy.
...
aws_instance.app_server: Creation complete after 24s [id=i-0358d1df58e055d70]
```

The output shows three resources were created – the AWS instance (EC2), an IP subnet, and a network VPC. Now, let’s take a look at the code (the complete code can be seen inside the `chapter14/simpleec2` directory). The code requires your AWS keys, storing them inside the `variable` block as `aws_access_key` and `aws_secret_key`:

```markup
terraform {
 ...
}
variable "aws_access_key" {
 type = string
}
variable "aws_secret_key" {
 type = string
}
provider "aws" {
 region     = "us-east-1"
 access_key = var.aws_access_key
 secret_key = var.aws_secret_key
}
```

The keys will be passed to the `aws` provider to enable the provider to communicate with the AWS service using our keys.

The following part of the code creates the VPC and IP subnet, which will be used as a private network by EC2 instances:

```markup
resource "aws_vpc" "default-vpc" {
 cidr_block           = "10.0.0.0/16"
 enable_dns_hostnames = true
 tags                 = {
   env = "dev"
 }
}
resource "aws_subnet" "default-subnet" {
 cidr_block = "10.0.0.0/24"
 vpc_id     = aws_vpc.default-vpc.id
}
```

The last resource the code defines is the EC2 instance, as follows:

```markup
resource "aws_instance" "app_server" {
 ami             = "ami-0ff8a91507f77f867"
 instance_type   = "t2.nano"
 subnet_id       = aws_subnet.default-subnet.id
 tags = {
   Name = "Chapter14"
 }
}
```

The EC2 instance type is `t2.nano`, which is the smallest virtual machine that can be configured. It is linked to the IP subnet defined earlier by assigning the subnet ID to the `subnet_id` parameter.

## Deploying to ECS with a load balancer

The last example that we are going to look at is using AWS ECS. The source code can be found inside the `chapter14/lbecs` directory. The code will use ECS to deploy our [_Chapter 12_](https://subscription.imaginedevops.io/book/web-development/9781803234199/2B18295_12.xhtml#_idTextAnchor241) container hosted in GitHub Packages and made scalable by using a load balancer. _Figure 14__.9_ shows the infrastructure configuration after running the code.

![Figure 14.10: ECS with a load balancer](https://static.packt-cdn.com/products/9781803234199/graphics/image/Figure_14.10_B18295.jpg)

Figure 14.10: ECS with a load balancer

The code uses the following services:

-   **An internet gateway**: As the name implies, this is a gateway that enables communication to be established between the AWS VPC private network and the internet. With the help of the gateway, we open our application to the world.
-   **A load balancer**: This service helps balance the incoming traffic across the different networks configured, ensuring that the application can take care of all incoming requests.

ECS provides the capability to scale the deployment process for containers. This means that, as developers, we don’t have to worry about how to scale the containers that are running our application, as this is all taken care of by ECS. More in-depth information can be found at [https://aws.amazon.com/ecs/](https://aws.amazon.com/ecs/). The application is run the same way as in the previous examples, using the `terraform init` and `terraform` `apply` commands.

Note

The ECS example takes a bit longer to execute compared to the other examples.

You will get output that looks like the following:

```markup
...
Terraform will perform the following actions:
  # aws_default_route_table.lbecs-subnet-default-route-
  # table will be created
  + resource "aws_default_route_table"
             "lbecs-subnet-default-route-table" {
      ...
    }
  # aws_ecs_cluster.lbecs-ecs-cluster will be created
  + resource "aws_ecs_cluster" "lbecs-ecs-cluster" {
      ...
    }
  # aws_ecs_service.lbecs-ecs-service will be created
  + resource "aws_ecs_service" "lbecs-ecs-service" {
      ...
    }
  # aws_ecs_task_definition.lbecs-ecs-task-definition will
  # be created
  + resource "aws_ecs_task_definition"
             "lbecs-ecs-task-definition" {
      ...
    }
  # aws_internet_gateway.lbecs-igw will be created
  + resource "aws_internet_gateway" "lbecs-igw" {
     ...
    }
  # aws_lb.lbecs-load-balancer will be created
  + resource "aws_lb" "lbecs-load-balancer" {
      ...
    }
  # aws_lb_listener.lbecs-load-balancer-listener will be
  # created
  + resource "aws_lb_listener"
             "lbecs-load-balancer-listener" {
      ...
    }
  # aws_lb_target_group.lbecs-load-balancer-target-group
  # will be created
  + resource "aws_lb_target_group"
             "lbecs-load-balancer-target-group" {
      ...
    }
  # aws_security_group.lbecs-security-group will be created
  + resource "aws_security_group" "lbecs-security-group" {
      ...
    }
  # aws_subnet.lbecs-subnet will be created
  + resource "aws_subnet" "lbecs-subnet" {
      ...
    }
  # aws_subnet.lbecs-subnet-1 will be created
  + resource "aws_subnet" "lbecs-subnet-1" {
      ...
    }
  # aws_vpc.lbecs-vpc will be created
  + resource "aws_vpc" "lbecs-vpc" {
      ...
    }
Plan: 12 to add, 0 to change, 0 to destroy.
...
aws_ecs_service.lbecs-ecs-service: Creation complete after 2m49s [id=arn:aws:ecs:us-east-1:860976549008:service/lbecs-ecs-cluster/lbecs-ecs-service]
...
Outputs:
url = "load-balancer-1956367690.us-east-1.elb.amazonaws.com"
```

Let’s break down the code to see how it uses ECS and configures the internet gateway, load balancer, and network. The following code shows the internet gateway declaration, which is simple enough as it requires to be attached to a VPC:

```markup
resource "aws_internet_gateway" "lbecs-igw" {
 vpc_id = aws_vpc.lbecs-vpc.id
 tags = {
   Name = "Internet Gateway"
 }
}
resource "aws_default_route_table" "lbecs-subnet-default-route-table" {
 default_route_table_id =
   aws_vpc.lbecs-vpc.default_route_table_id
 route {
   cidr_block = "0.0.0.0/0"
   gateway_id = "${aws_internet_gateway.lbecs-igw.id}"
 }
}
```

Besides that, the gateway will also be attached to a routing table declared inside the `aws_default_route_table` block. This is necessary as this tells the gateway how to route the incoming and outgoing traffic through the internal private VPC network.

Now that our internal private network can communicate to the internet via a gateway, we need to have network rules in place to ensure our network is secure, and this is done in the following code:

```markup
resource "aws_security_group" "lbecs-security-group" {
 name        = "allow_http"
 description = "Allow HTTP inbound traffic"
 vpc_id      = aws_vpc.lbecs-vpc.id
 egress {
   from_port   = 0
   to_port     = 0
   protocol    = "-1"
   cidr_blocks = ["0.0.0.0/0"]
 }
 ingress {
   description = "Allow HTTP for all"
   from_port   = 80
   to_port     = 3333
   protocol    = "tcp"
   cidr_blocks = ["0.0.0.0/0"]
 }
}
```

The `egress` block declares the rule for outgoing network traffic, allowing all protocols to pass through. The incoming network traffic rule is declared in the `ingress` block, and allows ports between `80`\-`3333` and only over TCP.

Using a load balancer requires two different subnets to be declared. In our code example, this is as follows:

```markup
resource "aws_lb" "lbecs-load-balancer" {
 name               = "load-balancer"
 internal           = false
 load_balancer_type = "application"
 security_groups    = [aws_security_group.lbecs-security-group.                       id]
 subnets            = [aws_subnet.lbecs-subnet.id,
                       aws_subnet.lbecs-subnet-1.id]
 tags               = {
   env = "dev"
 }
}
```

The last piece of code that we will look at is the ECS block, as follows:

```markup
resource "aws_ecs_cluster" "lbecs-ecs-cluster" {
 name = "lbecs-ecs-cluster"
}
resource "aws_ecs_task_definition" "lbecs-ecs-task-definition" {
 family                   = "service"
 requires_compatibilities = ["FARGATE"]
 network_mode             = "awsvpc"
 cpu                      = 1024
 memory                   = 2048
 container_definitions    = jsonencode([
   {
     name         = "lbecs-ecs-cluster-chapter14"
     image        =
       "ghcr.io/nanikjava/golangci/chapter12:latest"
     ...
     portMappings = [
       {
         containerPort = 3333
       }
     ]
   }
 ])
}
resource "aws_ecs_service" "lbecs-ecs-service" {
 name            = "lbecs-ecs-service"
 cluster         = aws_ecs_cluster.lbecs-ecs-cluster.id
 task_definition =
   aws_ecs_task_definition.lbecs-ecs-task-definition.arn
 desired_count   = 1
 launch_type     = "FARGATE"
 network_configuration {
   ...
 }
 load_balancer {
   target_group_arn = aws_lb_target_group.lbecs-load-
                      balancer-target-group.arn
   container_name   = "lbecs-ecs-cluster-chapter14"
   container_port   = 3333
 }
 tags = {
   env = "dev"
 }
}
```

The preceding code contains three different code blocks that are explained as follows:

-   `aws_ecs_cluster`: This block configures the name of the ECS cluster
-   `aws_ecs_task_definition`: This block configures the ECS task, which specifies what kind of container it has to run, the virtual machine configuration that the container will be running on, the network mode, security group, and other options
-   `aws_ecs_service`: This block ties together the different services to describe the complete infrastructure that will be run, such as security, ECS task, network configuration, load balancers, public IP address, and more

Once ECS has been spun up, it will print out in your console the load-balanced public address you can use to access the application. For example, when it was run, we got the following output in the terminal:

```markup
…
aws_lb_listener.lbecs-load-balancer-listener: Creating...
aws_lb_listener.lbecs-load-balancer-listener: Creation complete after 1s [id=arn:aws:elasticloadbalancing:us-east-1:860976549008:listener/app/load-balancer/4ad0f8b815a06f02/d945bba078d0c365]
aws_ecs_service.lbecs-ecs-service: Creation complete after 2m27s [id=arn:aws:ecs:us-east-1:860976549008:service/lbecs-ecs-cluster/lbecs-ecs-service]
Apply complete! Resources: 12 added, 0 changed, 0 destroyed.
Outputs:
url = "load-balancer-375816308.us-east-1.elb.amazonaws.com"
```

Using the `load-balancer-375816308.us-east-1.elb.amazonaws.com` address in the browser will show the application login page. This address is dynamically generated by AWS, and you will get something different than what is shown in the previous output.

Bookmark

# Summary

In this chapter, we explored cloud solutions provided by AWS, and we briefly looked at the different services offered, such as EC2, VPC, storage, and others. We learned about the open source Terraform tools that make it easy to create, manage, and destroy cloud infrastructure in AWS.

We learned how to install and use Terraform locally, and how to write Terraform code to use Docker as a provider, allowing us to run containers locally. Terraform also allows us to download, run, and destroy containers locally with a single command.

We also explored different Terraform examples for creating AWS infrastructure resources and looked at one of the advanced features of AWS ECS.

In this last chapter of the book, you have learned the different things that need to be done to deploy an application to the AWS cloud.